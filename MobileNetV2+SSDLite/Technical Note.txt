I wrote everything in ViewController.swift in order to implement the required functionalities as quickly as I can.

UIImagePickerController is used to let user pick a video file from photo library.
After a video file is picked, processing is moved off the main thread in order not to freeze UI.
The picked video file is available as a URL and loaded using AVAsset(url:).
Then, AVAssetReader with AVAssetReaderVideoCompositionOutput added is used to read the video file into CMSampleBuffer ready to be fed into the sample code's existing predict(sampleBuffer:) method to trigger object detection.
The results of object detection is available from the existing processObservations(for:error:); thus, I inserted detectPerson(_:) in the method to record the first frame index where a person is detected with confidence level >= 0.9.

By the way, although the VNRequest API appears asynchronous due to completion handler, the documentation for the perform(_:) method of VNImageRequestHandler says that "The function returns after all requests have either completed or failed." and I verified that they are actually executed synchronously.

After the frame data has been processed for object detection, the data is fed into an AVAssetWriterInput added to an AVAssetWriter to record an output video if needed.
Reading and writing the video files continues until 10 seconds of video is recorded (by counting the number of frames and frame rate), 5 seconds have passed since last detected person, or the end of the input video is reached.
Finally, the output video is moved from its temporary location in the app's Documents folder to the user's photo library using UISaveVideoAtPathToSavedPhotosAlbum(_:_:_:_:)
